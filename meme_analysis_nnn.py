# -*- coding: utf-8 -*-
"""Meme Analysis NNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mEDkVBW9DYH-u7K_L4OI1vkSg_yqyt1P
"""

import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import numpy as np
from skimage import color
from PIL import ImageFile
import skimage
from PIL import Image
from skimage.transform import resize
import pandas as pd
from skimage.io import imread, imshow
from nltk import WordNetLemmatizer
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models
from torch.utils.data import Dataset, DataLoader
#nltk.download('stopwords')
#nltk.download('wordnet')
#nltk.download('omw-1.4')
#nltk.download('punkt')

torch.set_default_tensor_type(torch.DoubleTensor)

"""## **Task1**"""

data=pd.read_csv('sentiment.csv',index_col=0)
data=data[data['filename']!='COCO_val2014_000000130712.jpg']
data=data[data['filename']!='COCO_val2014_000000359276.jpg']
data=data[data['filename']!='COCO_val2014_000000310622.jpg']
data=data[data['filename']!='COCO_val2014_000000421673.jpg']

data=data.reset_index(drop=True)
data

resnet=models.resnet18(pretrained=True)
num_ftrs = resnet.fc.in_features
resnet.fc = nn.Linear(num_ftrs, 512)
resnet = resnet.cuda()
img=list(data['filename'])

for i,filename in enumerate(img):
  image = Image.open(r'C:/Users/Ryzen/Downloads/sentiment/sentiment_images/'+f'{filename}')
  image = transforms.Resize((256, 256))(image)
  image=image.convert("RGB")
  image_tensor = transforms.ToTensor()(image)
  image_tensor = torch.unsqueeze(image_tensor, dim=0).to('cuda')
  image_tensor= resnet(image_tensor)
  torch.save(image_tensor,r"C:/Users/Ryzen/Downloads/sentiment/image_tensors/" + f'{i}.pt')

def remove_pun(st):             #remove punctuation form the string
    pun=[]
    for i in st:
        if(not i in string.punctuation):     #if no punctuation append in list
            pun.append(i)
    return pun
def remove_stopwords(word_list):
        st = set(stopwords.words('english'))
        word=[]
        for i in word_list:
            if not i.lower() in st:
                word.append(i);
        return word

def text_process(val):
  word=remove_pun(val)
  word=''.join(word)
  word=remove_stopwords(word.split(' '))
  word=' '.join(word)
  word=word.lower()

  lemmatizer = WordNetLemmatizer()
  word=lemmatizer.lemmatize(word)
  return word

from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer=tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
model=model.cuda()
text=list(data['raw'])
for i,val in enumerate(text):
    text[i]=text_process(val)
for i,t in enumerate(text):
    text_data = [t]

    tokens = tokenizer.batch_encode_plus(text_data, padding=True, truncation=True, return_tensors='pt')
    input_ids = tokens['input_ids'].to('cuda')
    attention_mask = tokens['attention_mask'].to('cuda')

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        embeddings = outputs.pooler_output

    torch.save(embeddings[0],r"C:/Users/Ryzen/Downloads/sentiment/text_tensors/" + f'{i}.pt')

import os
file_names = os.listdir(r"C:/Users/Ryzen/Downloads/sentiment/image_tensors/")
t_file=[]
for file_name in file_names:
    t_file.append(file_name)

t_file = sorted(t_file, key=lambda x: int(x.split('.')[0]))
t_file
data['t_file']=t_file

file_names = os.listdir(r"C:/Users/Ryzen/Downloads/sentiment/text_tensors/")
tt_file=[]
for file_name in file_names:
    tt_file.append(file_name)

tt_file = sorted(tt_file, key=lambda x: int(x.split('.')[0]))
data['tt_file']=tt_file

data.iloc[100]

dt=data[['tt_file','t_file','sentiment','split']]
dt2=data[['tt_file','t_file','sentiment','split']]
dt3=data[['tt_file','t_file','sentiment','split']]
dt=dt[dt['split'] == 'train']
dt2=dt2[dt2['split'] == 'test']
dt3=dt3[dt3['split'] == 'val']
dt=dt.drop("split", axis=1)
dt2=dt2.drop("split", axis=1)
dt3=dt3.drop("split", axis=1)
dt=dt.reset_index()
dt2=dt2.reset_index()
dt3=dt3.reset_index()

dt

dt2

dt3

class CustomDataset(Dataset):
    def __init__(self, df):
        self.df = df
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        index,tt_file,t_file,label = self.df.iloc[idx]
        image_tensor=torch.load(r"C:/Users/Ryzen/Downloads/sentiment/image_tensors/" + f'{t_file}')
        text_tensor=torch.load(r"C:/Users/Ryzen/Downloads/sentiment/text_tensors/" + f'{tt_file}')
        return image_tensor,text_tensor,label

d=CustomDataset(dt)

d[0]

train_set=CustomDataset(dt)
val_set=CustomDataset(dt3)
test_set=CustomDataset(dt2)

BATCH_SIZE = 256
train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)
val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)
test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)

len(test_dataloader)

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()

        # Load the ResNet32 model pretrained on ImageNet

        self.im_layer1 = nn.Linear(512, 256)
        self.im_layer2 = nn.Linear(256,128)
        self.im_layer3 = nn.Linear(128,64)
        self.im_layer4 = nn.Linear(64,16)

        self.text_fc1 = nn.Linear(768, 256)
        self.text_fc2 = nn.Linear(256, 100)
        self.text_fc3 = nn.Linear(100, 24)
        self.text_fc4 = nn.Linear(24, 16)

        self.linear1 = nn.Linear(32,32)
        self.linear2 = nn.Linear(32,24)
        self.linear3 = nn.Linear(24,16)
        self.linear4 = nn.Linear(16,1)

        # Sigmoid activation function
        self.sigmoid = nn.Sigmoid()
    def forward(self, image_features,text_features):
        # Forward pass for image features using ResNet32

        image_output = self.im_layer1(image_features)
        image_output = F.relu(image_output)

        image_output = self.im_layer2(image_output)
        image_output = F.relu(image_output)

        image_output = self.im_layer3(image_output)
        text_output = F.relu(image_output)

        image_output = self.im_layer4(image_output)
        # Forward pass for text features using linear layers
        text_output = self.text_fc1(text_features)
        text_output = F.relu(text_output)

        text_output = self.text_fc2(text_output)
        text_output = F.relu(text_output)

        text_output = self.text_fc3(text_output)
        text_output = F.relu(text_output)

        text_output = self.text_fc4(text_output)
        # Combine the image and text outputs
        combined_output = torch.cat((image_output,text_output),dim=1)

        # Apply sigmoid activation

        output = self.linear1(combined_output)
        output = F.relu(output)

        output = self.linear2(output)
        text_output = F.relu(output)

        output = self.linear3(output)
        text_output = F.relu(output)

        output = self.linear4(output)
        output = self.sigmoid(output)

        return output

model = MyModel()
for name, param in model.named_parameters():
    print(f"Parameter {name}, shape {param.shape}")

def validation(model, validloader):
    valid_loss = 0
    accuracy = 0
    criterion = nn.BCELoss()

    for i, (images,text, labels) in enumerate(validloader):
        images=images.to('cuda')
        text=text.to('cuda')
        labels=labels.to('cuda')

        output = model(images.squeeze(1),text)
        loss=criterion(outputs.flatten(), labels.double()).item()
        valid_loss+=loss.item*256

        accuracy += int(sum((outputs.flatten()>0.5)==labels.flatten()))


    return valid_loss, accuracy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
criterion = nn.BCELoss()
learning_rate=0.001
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
model = MyModel().to('cuda')
num_epochs=20

# Training loop
for epoch in range(num_epochs):
    model.train()
    torch.enable_grad()
    for batch_idx, (images,text, targets) in enumerate(train_dataloader):
        images = images.to(device)
        text = text.to(device)
        targets = targets.to(device)

        # Forward pass
        outputs = model(images.squeeze(1),text)
        loss = criterion(outputs.flatten(), targets.double())
        trainLoss+=loss.item*256
        trainAccuracy += int(sum((outputs.flatten()>0.5)==targets.flatten()))

        model.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation loop
    model.eval()
    valLoss = 0
    valAccuracy = 0

    valLoss,valAccuracy=validation(model,val_dataloader)

    print(f'Epoch # {epoch + 1}:-')
    print(f'Train Acc:- {trainAccuracy  / len(train_dataloader): .5f}   Train Loss:- {trainLoss / len(train_dataloader): .5f}')
    print(f'Val Loss:- {valLoss / len(val_dataloader): .5f}   Val Acc:- {valAccuracy  / len(val_dataloader): .5f}')

outputs=[]
labelss=[]
learningRate = 0.001
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr= learnRate)
device = torch.device("cuda" if useCuda else "cpu")
with torch.no_grad():
    for image, text, labels in test_dataloader:
        image = image.to(device)
        text = text.to(device)
        labels = labels.to(device)

        outputs = model(image.squeeze(1),text)
        loss = criterion(output.flatten(), labels.double())
        test_loss += loss.item()*256
        accu = ((output.flatten() >= 0.5) == labels).sum().item()
        Test_accu += accu
        outputs.append((output.flatten() >= 0.5).tolist())
        labelss.append(valLabel.tolist())

        print(f'Test Loss:- {test_loss/len(test_dataloader): .5f}   Test Acc:- {Test_accu  / len(test_dataloader): .5f}')

outputs
outputss=[]
for i in outputs:
    for j in i:
        if j==True:
            outputss.append(1)
        else:
            outputss.append(0)

labelss
labelsss=[]
for i in labelss:
    for j in i:
        labelsss.append(j)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sn
print(classification_report(labelsss,outputss))
cf=confusion_matrix(labelsss, outputss)
sn.heatmap(cf,annot=True,fmt='.0f')

pip install torchviz

from torchviz import make_dot
model = MyModel()
model = model.to('cuda:0')
t_dataloader = DataLoader(train_set, batch_size=1, shuffle=True,drop_last=True)
train_img,train_text, train_labels = next(iter(t_dataloader))
train_img = train_img.to('cuda:0')
train_text = train_text.to('cuda:0')
analysis = model(train_img.squeeze(1),train_text)
make_dot(analysis,params=dict(list(model.named_parameters())))

"""### **Task 2:**"""

data

data2 = data.drop_duplicates(subset='raw')

data2=data2.reset_index(drop=True)

from nltk import ngrams
from nltk.tokenize import word_tokenize
text=list(data2['tokens'])
split=list(data2['split'])
splits=[]
ngrams=[]
for idx,j in enumerate(text):
    tokens = eval(j)
    for i in range(len(tokens)):
        if  i==len(tokens)-1:
           ngram = [tokens[i-1], tokens[i], ' ']
        else:
          if i-1>=0:
            ngram = [tokens[i-1], tokens[i], tokens[i + 1]]
          if i-1<0:
            ngram = [' ', tokens[i], tokens[i + 1]]
        splits.append(split[idx])
        ngrams.append(ngram)

ngrams

data3=pd.DataFrame()
data3['ngramss']=ngrams
data3['split']=splits
data3

label=list(data2['word_sentiment'])
labels=[]
for i,lbl in enumerate(label):
    lbl=eval(lbl)
    for j in lbl:
        labels.append(j)

data3['label']=labels
data3.head(10)

data2['label']=label

from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer=tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
model=model.cuda()
ngrams=list(data3['ngramss'])
for i,t in enumerate(ngrams):
        t=' '.join(t)
        text_data = [t]

        tokens = tokenizer.batch_encode_plus(text_data, padding=True, truncation=True, return_tensors='pt')
        input_ids = tokens['input_ids'].to('cuda')
        attention_mask = tokens['attention_mask'].to('cuda')

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            embeddings = outputs.pooler_output
        torch.save(embeddings[0],r"C:/Users/Ryzen/Downloads/sentiment/text_tensors2/" + f'{i}.pt')

file_names = os.listdir(r"C:/Users/Ryzen/Downloads/sentiment/text_tensors2/")
tt2_file=[]
for file_name in file_names:
    tt2_file.append(file_name)

tt2_file = sorted(tt2_file, key=lambda x: int(x.split('.')[0]))
data3['t_file']=tt2_file

data3

text1=data3[['split','label','t_file']]
text2=data3[['split','label','t_file']]
text3=data3[['split','label','t_file']]
text1=text1[text1['split'] == 'train']
text2=text2[text2['split'] == 'test']
text3=text3[text3['split'] == 'val']
text1=text1.drop("split", axis=1)
text2=text2.drop("split", axis=1)
text3=text3.drop("split", axis=1)

text1

text2

text3

class CustomDataset2(Dataset):
    def __init__(self, df):
        self.df = df
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        labels,ngrams = self.df.iloc[idx]
        ngrams=torch.load(r"C:/Users/Ryzen/Downloads/sentiment/text_tensors2/" + f'{ngrams}')
        return ngrams,torch.tensor(labels).double()

d=CustomDataset2(text1)

d[0]

train_set=CustomDataset2(text1)
val_set=CustomDataset2(text3)
test_set=CustomDataset2(text2)

BATCH_SIZE = 256
train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)
val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)
test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)

class Text_Model(nn.Module):
    def __init__(self):
        super(Text_Model, self).__init__()

        self.linear1 = nn.Linear(768,1100)
        self.linear2 = nn.Linear(1100,512)
        self.linear3 = nn.Linear(512,256)
        self.linear4 = nn.Linear(256,128)
        self.linear5 = nn.Linear(128,64)
        self.linear6 = nn.Linear(64,32)
        self.linear7 = nn.Linear(32,1)


        self.sigmoid = nn.Sigmoid()
    def forward(self,text_features):

        output = self.linear1(text_features)
        output = F.relu(output)

        output = self.linear2(output)
        text_output = F.relu(output)

        output = self.linear3(output)
        text_output = F.relu(output)

        output = self.linear4(output)
        output = F.relu(output)

        output = self.linear5(output)
        output = F.relu(output)

        output = self.linear6(output)
        output = F.relu(output)

        output = self.linear7(output)
        output = self.sigmoid(output)

        return output

model = MyModel()
for name, param in model.named_parameters():
    print(f"Parameter {name}, shape {param.shape}")

def validation2(model, validloader):
    valid_loss = 0
    accuracy = 0
    criterion = nn.BCELoss()

    for i, (text, labels) in enumerate(validloader):
        text=text.to('cuda')
        labels=labels.to('cuda')

        output = model(text)
        loss=criterion(outputs.flatten(), labels.double()).item()
        valid_loss+=loss.item*256

        accuracy += int(sum((outputs.flatten()>0.5)==labels.flatten()))

    return valid_loss, accuracy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
criterion = nn.BCELoss()
learning_rate=0.001
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
model = MyModel().to('cuda')
num_epochs=10

# Training loop
for epoch in range(num_epochs):
    model.train()
    torch.enable_grad()
    for idx, (text, targets) in enumerate(train_dataloader):
        text = text.to(device)
        targets = targets.to(device)

        # Forward pass
        outputs = model(text)
        loss = criterion(outputs.flatten(), targets)
        train_Loss+=loss.item*256
        train_Accuracy += int(sum((outputs.flatten()>0.5)==targets.flatten()))

        model.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation loop
    model.eval()
    valLoss = 0
    valAccuracy = 0

    valLoss,valAccuracy=validation2(model,val_dataloader)

    print(f'Epoch # {epoch + 1}:-')
    print(f'Train Acc:- {train_Accuracy  / len(train_dataloader): .5f}   Train Loss:- {train_Loss / len(train_dataloader): .5f}')
    print(f'Val Loss:- {valLoss / len(val_dataloader): .5f}   Val Acc:- {valAccuracy  / len(val_dataloader): .5f}')

outputs=[]
labelss=[]
learningRate = 0.001
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr= learnRate)
device = torch.device("cuda" if useCuda else "cpu")
with torch.no_grad():
    for  text, labels in test_dataloader:

        text = text.to(device)
        labels = labels.to(device)

        outputs = model(text)
        loss = criterion(output.flatten(), labels)
        test_Loss += loss.item()*256
        accu = ((output.flatten() >= 0.5) == labels).sum().item()
        Test_accuracy += accu
        outputs.append((output.flatten() >= 0.5).tolist())
        labelss.append(valLabel.tolist())

        print(f'Test Loss:- {test_Loss/len(test_dataloader): .5f}   Test Acc:- {Test_accuracy/len(test_dataloader): .5f}')

labelsss=[]
for i in labelss:
    for j in i:
        labelsss.append(j)

outputs
outputss=[]
for i in outputs:
    for j in i:
        if j==True:
            outputss.append(1)
        else:
            outputss.append(0)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sn
print(classification_report(labelsss,outputss))
cf=confusion_matrix(labelsss, outputss)
sn.heatmap(cf,annot=True,fmt='.0f')

from torchviz import make_dot
model = Text_Model()
model = model.to('cuda:0')
t_dataloader = DataLoader(train_set, batch_size=1, shuffle=True,drop_last=True)
train_text, train_labels = next(iter(t_dataloader))
train_text = train_text.to('cuda:0')
analysis = model(train_text)
make_dot(analysis,params=dict(list(model.named_parameters())))

